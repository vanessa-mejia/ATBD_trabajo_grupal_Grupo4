#Consulta No. 2: Promedio de la duración de las canciones por década
Permite identificar si ha habido una tendencia en cuanto a la duración de las canciones, lo que puede estar relacionado con los cambios en los gustos del público, la evolución de los formatos musicales y la industria de la música.



from pyspark.sql import SparkSession
from dotenv import load_dotenv
import os


#Se realiza la carga las variables de entorno
APP_NAME = "Promedio_Duracion_Canciones_Por_Decada"


load_dotenv()


# Directorios de entrada y salida
INPUT_DIR = str(os.getenv("DATA_DIR"))
OUTPUT_DIR = str(os.getenv("OUTPUT_DIR"))


# Se crea la sesión de Spark
spark = SparkSession.builder \
    .appName(APP_NAME) \
    .master("yarn") \
    .getOrCreate()


# Ruta de entrada y salida
data_dir = INPUT_DIR
output_dir = f"{OUTPUT_DIR}/{APP_NAME}/"


#Se leen los datos desde HDFS
df = spark.read.option("header", "true") \
    .option("delimiter", ",") \
    .option("encoding", "latin1") \
    .csv(data_dir)


# Se verifican los tipos de datos
df = df.withColumn("AÑO", col("AÑO").cast("integer")) \
       .withColumn("DURACION", col("DURACION").cast("float"))


# Se crear una columna de década
df = df.withColumn("DECADA", (df["AÑO"] // 10) * 10)


# Se agrupan los datos por década y calcular el promedio de duración de las canciones
df_aggregated = df.groupBy("DECADA").agg(spark_avg("DURACION").alias("PROMEDIO_DURACION"))


# Se ordenan los resultados por década
df_aggregated = df_aggregated.orderBy("DECADA")


#Se almacenar los resultados en HDFS
df_aggregated.write.mode("overwrite").option("header", "true").csv(output_dir)


# Mostrar los resultados
df_aggregated.show(truncate=False)


# Finalizar sesión
spark.stop()
